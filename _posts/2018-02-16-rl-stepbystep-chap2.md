---
layout: post
category: "rl"
title: "深入浅出强化学习-chap2 马尔科夫决策过程"
tags: [深入浅出强化学习, 马尔科夫决策过程 ]
---

目录

<!-- TOC -->

- [1. 马尔科夫决策过程理论](#1-马尔科夫决策过程理论)
    - [1.1 马尔科夫性](#11-马尔科夫性)
    - [1.2 马尔科夫过程](#12-马尔科夫过程)
    - [1.3 马尔科夫决策过程](#13-马尔科夫决策过程)
    - [1.4 状态值函数与状态-行为值函数](#14-状态值函数与状态-行为值函数)
    - [1.5 状态值函数与状态-行为值函数的贝尔曼方程](#15-状态值函数与状态-行为值函数的贝尔曼方程)
- [2. MDP中的概率学基础](#2-mdp中的概率学基础)
    - [期望与方差](#期望与方差)
        - [期望](#期望)
        - [方差](#方差)
- [3. 基于gym的MDP实例](#3-基于gym的mdp实例)

<!-- /TOC -->



参考**《深入浅出强化学习》**

## 1. 马尔科夫决策过程理论

### 1.1 马尔科夫性

系统的下一个状态`\(s_{t+1}\)`**仅与当前状态`\(s_t\)`有关**，与之前的状态无关。

定义：状态`\(s_t\)`是马尔科夫的，当且仅当`\(P[s_{t+1}|s_t]=P[s_{t+1}|s_1,...,s_t]\)`。

随机过程就是随机变量序列。如果随机变量序列的**每个状态**都是**马尔科夫**的，那么此随机过程就是**马尔科夫随机过程**。

### 1.2 马尔科夫过程

马尔科夫过程是一个二元组`\((S,P)\)`，且满足：`\(S\)`是有限状态集合，`\(P\)`是状态转移概率。状态转移概率矩阵如下：

`\[
P=\begin{bmatrix}
P_{11} & ... & P_{1n} \\ 
\vdots  & \vdots  & \vdots \\ 
P_{n1} & ... & P_{nn}
\end{bmatrix}
\]`

由若干个状态组成的序列称为马尔科夫链。当给定状态转移概率时，从某个状态出发存在多条马尔科夫链。

### 1.3 马尔科夫决策过程

马尔科夫决策过程由元组`\((S,A,P,R,\gamma)\)`描述，其中：

+ `\(S\)`为有限的状态集
+ `\(A\)`为有限的动作集
+ `\(P\)`为状态转移概率
+ `\(R\)`为回报函数
+ `\(\gamma\)`为**折扣因子**，用于计算**累积回报**

区别于马尔科夫过程，马尔科夫决策过程的**态转移概率**是**包含动作**的，即`\(P^a_{ss'}=P[S_{t+1}=s'|S_t=s,A_t=a]\)`。

强化学习的目标是给定一个马尔科夫决策过程，寻找最优策略。**策略**指的是，**状态到动作的映射**，用符号`\(\pi\)`表示，给定状态`\(s\)`时，动作集上的一个分布，即：

`\[
\pi (a|s)=p[A_i=a|S_t=s]
\]`

含义是，策略`\(\pi\)`在每个状态`\(s\)`指定一个动作概率。如果给出的策略`\(\pi\)`是确定性的，那么策略`\(\pi\)`在每个状态`\(s\)`指定一个确定的动作。

概率在强化学习中的作用：

+ 强化学习的策略往往是随机策略。可以将**探索**耦合到**采样**的过程中。**探索，指机器人尝试其他动作以便找到更好的策略**。
+ 实际应用中，存在各种噪声，这些噪声大都服从正态分布，可以用概率的知识**去掉噪声**。

**给定一个策略`\(\pi\)`**时，可以计算**累积回报**：

`\[
G_t=R_{t+1}+\gamma R_{t+2}+...=\sum _{k=0}^{\infty }\gamma ^kR_{i+k+1}
\]`

当给定策略`\(\pi\)`时，假设从状态`\(s_t\)`出发，可能有很多个状态序列，所以对应的`\(G_t\)`也有多个可能值。为了评价状态`\(s_t\)`的价值，需要一个确定的量来描述。而**累积回报`\(G_t\)`是一个随机变量**，不是确定值，但其**期望**是一个**确定值**，因此可以做为状态值函数的定义。

### 1.4 状态值函数与状态-行为值函数

当智能体**采用策略`\(\pi\)`时**，累积回报服从一个分布，**累积回报**在**状态`\(s\)`处的期望值**定义**状态值函数**：

`\[
\upsilon _{\pi}(s)=E_{\pi}[\sum _{k=0}^{\infty }\gamma ^kR_{t+k+1}|S_t=s]
\]`

注：**状态值函数是与策略`\(\pi\)`相对应的**，因为策略`\(\pi\)`决定了累积回报`\(G\)`的状态分布。

状态-行为值函数为：

`\[
q_{\pi}(s,a)=E_{\pi}[\sum _{k=0}^{\infty}\gamma ^kR_{t+k+1}|S_t=s,A_t=a]
\]`

### 1.5 状态值函数与状态-行为值函数的贝尔曼方程

贝尔曼方程（Bellman Equation）也被称作动态规划方程（Dynamic Programming Equation）。

状态值函数的贝尔曼方程：

`\[
\\\upsilon(S_t)=\upsilon(s)=E[G_t|S_t=s]
\\=E[R_{t+1}+\gamma R_{t+2}+...|S_t=s]
\\=E[R_{t+1}+\gamma (R_{t+2}+\gamma R_{t+3}+...)|S_t=s]
\\=E[R_{t+1}+\gamma G_{t+1}|S_t=s]
\\=E[R_{t+1}+\gamma \upsilon(S_{t+1})|S_t=s]
\]`

最后一个等号的证明（看书P23……）

`\[
\\\upsilon(S_t)=E
\]`


同理，状态-动作值函数的贝尔曼方程：

`\[
q_{\pi}(s,a)=E_{\pi}[R_{t+1}+\gamma q(S_{t+1},A_{t+1})|S_t=s,A_t=a]
\]`

## 2. MDP中的概率学基础

### 期望与方差

#### 期望

函数f(x)关于某分布P(x)的期望指，当x由分布P(x)产生，f作用于x时，f(x的平均值。

+ 离散型随机变量的期望：

`\[
E_{x~P}[f(x)]=\sum _xP(x)f(x)
\]`

+ 连续型随机变量的期望通过积分求得：

`\[
E_{x~P}[f(x)]=\int p(x)f(x)dx
\]`

期望的运算是线性的：

`\[
E_x(\alpha f(x)+\beta g(x))=\alpha E_x[f(x)]+\beta E_x[g(x)]
\]`

#### 方差

方差衡量利用当前概率分布采样时，采样值差异的大小：

`\[
Var(f(x))=E[(f(x)-E[f(x)])^2]
\]`

方差越小，不确定性越小。

## 3. 基于gym的MDP实例
