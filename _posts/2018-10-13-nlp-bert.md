---
layout: post
category: "nlp"
title: "bert"
tags: [bert, ]
---

目录

<!-- TOC -->

- [概述](#概述)
- [BERT](#bert)
    - [模型架构](#模型架构)
    - [Input Representation](#input-representation)
    - [Pre-training Tasks](#pre-training-tasks)
        - [Task #1: Masked LM](#task-1-masked-lm)
        - [Task #2: Next Sentence Prediction](#task-2-next-sentence-prediction)
    - [Pre-training Procedure](#pre-training-procedure)
    - [Fine-tuning Procedure](#fine-tuning-procedure)
    - [Comparison of BERT and OpenAI GPT](#comparison-of-bert-and-openai-gpt)
- [实验](#实验)
    - [GLUE Datasets](#glue-datasets)
        - [GLUE Results](#glue-results)
    - [SQuAD v1.1](#squad-v11)
    - [Named Entity Recognition](#named-entity-recognition)
    - [SWAG](#swag)
- [Ablation Studies](#ablation-studies)
    - [Effect of Pre-training Tasks](#effect-of-pre-training-tasks)
    - [Effect of Model Size](#effect-of-model-size)
    - [Effect of Number of Training Steps](#effect-of-number-of-training-steps)
    - [Feature-based Approach with BERT](#feature-based-approach-with-bert)

<!-- /TOC -->

参考[最强NLP预训练模型！谷歌BERT横扫11项NLP任务记录](https://www.jiqizhixin.com/articles/2018-10-12-13)

参考[https://www.zhihu.com/question/298203515/answer/509703208](https://www.zhihu.com/question/298203515/answer/509703208)

## 概述

本文介绍了一种新的语言表征模型BERT——来自**Transformer的双向编码器表征**。与最近的语言表征模型不同，BERT旨在基于**所有层**的**左、右语境**来预训练深度双向表征。BERT是首个在**大批句子层面**和**token层面**任务中取得当前最优性能的**基于微调的表征模型**，其性能超越许多使用任务特定架构的系统，刷新了11项NLP任务的当前最优性能记录。

目前将预训练语言表征应用于下游任务存在两种策略：feature-based的策略和fine-tuning策略。

+ feature-based策略（如 ELMo）使用**将预训练表征**作为**额外特征**的任务专用架构。
+ fine-tuning策略（如生成预训练 Transformer (OpenAI GPT)）引入了**任务特定最小参数**，通过**简单地微调预训练参数**在下游任务中进行训练。


在之前的研究中，两种策略在**预训练期间使用相同的目标函数**，利用**单向语言模型**来学习通用语言表征。

作者认为现有的技术严重制约了预训练表征的能力，微调策略尤其如此。其主要局限在于**标准语言模型是单向**的，这**限制了可以在预训练期间使用的架构类型**。例如，OpenAI GPT使用的是**从左到右**的架构，其中**每个token只能注意Transformer自注意力层中的先前token**。这些局限对于**句子层面的任务**而言不是最佳选择，对于**token级任务**（如 SQuAD 问答）则可能是毁灭性的，**因为在这种任务中，结合两个方向的语境至关重要**。

BERT（Bidirectional Encoder Representations from Transformers）改进了**基于微调的策略**。

BERT提出一种新的**预训练目标**——**遮蔽语言模型（masked language model，MLM）**，来克服上文提到的单向局限。MLM 的灵感来自 Cloze 任务（Taylor, 1953）。MLM**随机遮蔽输入中的一些token**，目标在于**仅基于遮蔽词的语境**来**预测其原始词汇id**。与从左到右的语言模型预训练不同，MLM目标**允许表征融合左右两侧的语境**，从而**预训练一个深度双向Transformer**。除了 MLM，我们还引入了一个**"下一句预测"（next sentence prediction）任务**，该任务**联合预训练**文本对表征。

贡献：

+ 展示了**双向预训练**语言表征的重要性。不同于 Radford 等人（2018）使用**单向语言模型进行预训练**，BERT使用MLM预训练深度双向表征。本研究与 Peters 等人（2018）的研究也不同，后者使用的是**独立训练**的**从左到右和从右到左LM**的**浅层级联**。
+ 证明了**预训练表征**可以**消除对许多精心设计的任务特定架构的需求**。BERT是首个在大批句子层面和token层面任务中取得当前最优性能的基于微调的表征模型，其性能超越许多使用任务特定架构的系统。
+ BERT 刷新了11项NLP任务的当前最优性能记录。本论文还报告了BERT的模型简化测试（ablation study），证明该模型的**双向特性是最重要的一项新贡献**。代码和预训练模型将发布在[goo.gl/language/bert](goo.gl/language/bert)。

## BERT

### 模型架构

BERT 旨在基于所有层的左、右语境来预训练深度双向表征。因此，预训练的 BERT 表征可以仅用一个额外的输出层进行微调，进而为很多任务（如问答和语言推断任务）创建当前最优模型，无需对任务特定架构做出大量修改。

BERT 的模型架构是一个多层双向Transformer编码器，基于Vaswani 等人 (2017)描述的原始实现，在tensor2tensor库中发布(当然，可以抽空看看[https://daiwk.github.io/posts/platform-tensor-to-tensor.html](https://daiwk.github.io/posts/platform-tensor-to-tensor.html)和[https://daiwk.github.io/posts/platform-tensor-to-tensor-coding.html](https://daiwk.github.io/posts/platform-tensor-to-tensor-coding.html))。

本文中，我们将**层数（即Transformer块）**表示为`\(L\)`，将**隐层的size**表示为`\(H\)`、**自注意力头数**表示为`\(A\)`。在所有实验中，我们将feed-forward/filter的size设置为`\(4H\)`，即H=768时为3072，H=1024时为4096。我们主要看下在两种模型尺寸上的结果：

+ **`\(BERT_{BASE}\)`**: L=12, H=768, A=12, Total Parameters=110M
+ **`\(BERT_{LARGE}\)`**:  L=24, H=1024, A=16, Total Parameters=340M

其中，`\(BERT_{BASE}\)`和OpenAI GPT的大小是一样的。BERT Transformer使用双向自注意力机制，而GPT Transformer使用受限的自注意力机制，导致每个token只能关注其左侧的语境。**双向Transformer**在文献中通常称为**“Transformer 编码器”**，而只**关注左侧语境的版本**则因能用于文本生成而被称为**“Transformer 解码器”**。

下图显示了BERT/GPT Transformer/ELMo的结构区别：

<html>
<br/>
<img src='../assets/bert-gpt-transformer-elmo.png' style='max-height: 300px'/>
<br/>
</html>

+ BERT 使用双向 Transformer
+ OpenAI GPT 使用从左到右的 Transformer
+ ELMo 使用独立训练的从左到右和从右到左LSTM的级联来生成下游任务的特征。

三种模型中，只有BERT表征会基于**所有层中的左右两侧语境**。

### Input Representation

论文的输入表示（input representation）能够在一个token序列中明确地表示**单个文本句子**或**一对文本句子**（例如， [Question, Answer]）。对于给定token，其输入表示通过对相应的token、segment和position embeddings进行求和来构造：

<html>
<br/>
<img src='../assets/bert-input-representation.png' style='max-height: 300px'/>
<br/>
</html>

+ 使用WordPiece嵌入【GNMT，[Google’s neural machine translation system: Bridging the gap between human and machine translation](https://arxiv.org/abs/1609.08144)】和30,000个token的词汇表。用##表示分词。
+ 使用learned positional embeddings，支持的序列长度最多为512个token。
+ 每个序列的第一个token始终是特殊分类嵌入（[CLS]）。对应于该token的最终隐藏状态（即，Transformer的输出）被用作分类任务的聚合序列表示。对于非分类任务，将忽略此向量。
+ 句子对被打包成一个序列。以两种方式区分句子。
  + 首先，用特殊标记（[SEP]）将它们分开。
  + 其次，添加一个learned sentence A嵌入到第一个句子的每个token中，一个sentence B嵌入到第二个句子的每个token中。
+ 对于单个句子输入，只使用 sentence A嵌入。


### Pre-training Tasks

+ 它在训练双向语言模型时以减小的概率把少量的词替成了Mask或者另一个随机的词。感觉其目的在于使模型被迫增加对上下文的记忆。（知乎的回答）
+ 增加了一个预测下一句的loss。

#### Task #1: Masked LM

#### Task #2: Next Sentence Prediction

### Pre-training Procedure

### Fine-tuning Procedure

### Comparison of BERT and OpenAI GPT

## 实验

### GLUE Datasets

#### GLUE Results

### SQuAD v1.1

### Named Entity Recognition

### SWAG

## Ablation Studies

### Effect of Pre-training Tasks

### Effect of Model Size

### Effect of Number of Training Steps

### Feature-based Approach with BERT


